{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "import random\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.mobilenet import MobileNet\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Flatten,Dense,Dropout,Conv2D,MaxPool2D,BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras import regularizers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7642ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindir = \"./dataset_final/Train\"\n",
    "validdir = \"./dataset_final/Validation\"\n",
    "os.listdir(traindir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b37acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(dir):\n",
    "    plt.figure(figsize=(12,7))\n",
    "    img = load_img(os.path.join(dir))\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c3b676",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "height = 120\n",
    "width = 120\n",
    "\n",
    "train_datagen =  ImageDataGenerator(rescale = 1/255.0,rotation_range=45,height_shift_range=0.2,shear_range=0.2,\n",
    "                              zoom_range=0.2,validation_split=0.2,horizontal_flip=True)\n",
    "\n",
    "train_data_gray = train_datagen.flow_from_directory(directory = traindir,target_size=(height,width),\n",
    "                                               color_mode='grayscale',\n",
    "                                               class_mode = \"categorical\",batch_size=32,subset=\"training\")\n",
    "\n",
    "val_data_gray = train_datagen.flow_from_directory(directory = validdir,target_size=(height,width),\n",
    "                                               class_mode = \"categorical\",batch_size=32,subset=\"validation\")\n",
    "\n",
    "\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1/255.0)\n",
    "test_data_gray = test_datagen.flow_from_directory(directory = validdir,target_size=(height,width),\n",
    "                                               class_mode = \"categorical\",batch_size=32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72900067",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_im(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalized data\n",
    "    \"\"\"\n",
    "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be464c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, os.path\n",
    "\n",
    "imgs = []\n",
    "path = \"./dataset_final/Train/Male\"\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "for f in tqdm(os.listdir(path)):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    im = Image.open(os.path.join(path,f)).convert('L')\n",
    "    im = np.array(im)\n",
    "\n",
    "    my_image = im\n",
    "    my_image = color_grayscale_arr_v2(my_image,\"green\")\n",
    "    my_image = my_image.reshape((1, my_image.shape[0], my_image.shape[1], my_image.shape[2]))\n",
    "    my_image = preprocess_input(my_image)\n",
    "    my_image = my_image.reshape((my_image.shape[1], my_image.shape[2],3))\n",
    "    my_image = normalize_im(my_image)\n",
    "\n",
    "    plt.imsave(\"./dataset_final/Train_Copy/Male/\"+f, my_image)\n",
    "\n",
    "#     imgs.append(Image.open(os.path.join(path,f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf223fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, os.path\n",
    "\n",
    "imgs = []\n",
    "path = \"./dataset_final/Train/Female\"\n",
    "valid_images = [\".jpg\",\".gif\",\".png\",\".tga\"]\n",
    "for f in tqdm(os.listdir(path)):\n",
    "    ext = os.path.splitext(f)[1]\n",
    "    if ext.lower() not in valid_images:\n",
    "        continue\n",
    "    im = Image.open(os.path.join(path,f)).convert('L')\n",
    "    im = np.array(im)\n",
    "\n",
    "    my_image = im\n",
    "    my_image = color_grayscale_arr_v2(my_image,\"green\")\n",
    "    my_image = my_image.reshape((1, my_image.shape[0], my_image.shape[1], my_image.shape[2]))\n",
    "    my_image = preprocess_input(my_image)\n",
    "    my_image = my_image.reshape((my_image.shape[1], my_image.shape[2],3))\n",
    "    my_image = normalize_im(my_image)\n",
    "\n",
    "    plt.imsave(\"./dataset_final/Train_Copy/Female/\"+f, my_image)\n",
    "\n",
    "#     imgs.append(Image.open(os.path.join(path,f)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0411f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_grayscale_arr_v2(arr, color='white'):\n",
    "    \"\"\"Converts grayscale image to either red or green\"\"\"\n",
    "    assert arr.ndim == 2\n",
    "    dtype = arr.dtype\n",
    "    h, w = arr.shape\n",
    "    arr = np.reshape(arr, [h, w, 1])\n",
    "    if (color=='red'):\n",
    "        arr = np.concatenate([arr,\n",
    "                          np.zeros((h, w, 2), dtype=dtype)], axis=2)\n",
    "    elif (color=='green'):\n",
    "        arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
    "                          arr,\n",
    "                          np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
    "    elif (color=='blue'):\n",
    "        arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
    "                          np.zeros((h, w, 1), dtype=dtype), \n",
    "                         arr], axis=2)\n",
    "    elif (color=='white'):\n",
    "        arr = np.concatenate([arr,\n",
    "                          arr, \n",
    "                         arr], axis=2)\n",
    "        \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9349d795",
   "metadata": {},
   "outputs": [],
   "source": [
    "def color_grayscale_arr(arr, color='white'):\n",
    "    \"\"\"Converts grayscale image to either red or green\"\"\"\n",
    "    assert arr.ndim == 3\n",
    "    dtype = arr.dtype\n",
    "    h, w,_ = arr.shape\n",
    "    arr = np.reshape(arr, [h, w, 1])\n",
    "    if (color=='red'):\n",
    "        arr = np.concatenate([arr,\n",
    "                          np.zeros((h, w, 2), dtype=dtype)], axis=2)\n",
    "    elif (color=='green'):\n",
    "        arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
    "                          arr,\n",
    "                          np.zeros((h, w, 1), dtype=dtype)], axis=2)\n",
    "    elif (color=='blue'):\n",
    "        arr = np.concatenate([np.zeros((h, w, 1), dtype=dtype),\n",
    "                          np.zeros((h, w, 1), dtype=dtype), \n",
    "                         arr], axis=2)\n",
    "    elif (color=='white'):\n",
    "        arr = np.concatenate([arr,\n",
    "                          arr, \n",
    "                         arr], axis=2)\n",
    "        \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5251d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_colored_data(init_dataset_gray,color,nb_samples):\n",
    "    samples = []\n",
    "    labels = []\n",
    "    count = 0\n",
    "    for gray_sample in train_data_gray:\n",
    "        my_image = gray_sample[0][0]\n",
    "        label = gray_sample[1][0]\n",
    "        my_image = color_grayscale_arr(my_image,color)\n",
    "        my_image = my_image.reshape((1, my_image.shape[0], my_image.shape[1], my_image.shape[2]))\n",
    "        my_image = preprocess_input(my_image)\n",
    "        my_image = my_image.reshape((my_image.shape[1], my_image.shape[2],3))\n",
    "        samples.append(my_image)\n",
    "        labels.append(label)\n",
    "        count = count + 1\n",
    "        if(count == nb_samples):\n",
    "            break\n",
    "    return np.array(samples),np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecad8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_pics = 18000\n",
    "\n",
    "X_gray_samples, Y_gray_samples = generate_colored_data(train_data_gray,\"white\",nb_pics)\n",
    "print(\"Done Gray\")\n",
    "X_green_samples, Y_green_samples = generate_colored_data(train_data_gray,\"green\",nb_pics)\n",
    "print(\"Done Green\")\n",
    "X_blue_samples, Y_blue_samples = generate_colored_data(train_data_gray,\"blue\",nb_pics)\n",
    "print(\"Done Blue\")\n",
    "X_red_samples, Y_red_samples = generate_colored_data(train_data_gray,\"red\",nb_pics)\n",
    "print(\"Done Red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b36391",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gray_samples.shape\n",
    "Y_gray_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b480e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(8, (3, 3), activation='relu', input_shape=(height,width,3)))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPool2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dense(2,activation = \"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06987209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(lr=0.001),loss = \"categorical_crossentropy\",metrics =[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data.samples\n",
    "batch_size = 32\n",
    "len(X_green_samples)// batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ba14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_green_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bcd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_green_samples, Y_green_samples, epochs=10,batch_size=256, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53845075",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(model.layers)):\n",
    "    layer = model.layers[i]\n",
    "    \n",
    "    print(i, layer.name, layer.output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435d4a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers[:12]] \n",
    "# Extracts the outputs of the top 12 layers\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This func will compute layerwise average activations and the maximum of these means\n",
    "def compute_avg_activation(activations,normalize=False):\n",
    "    nb_layers = len(activations)\n",
    "    layers_means = []\n",
    "    lambdas = []\n",
    "    for l in range(nb_layers):\n",
    "        layer_output = activations[l][0]\n",
    "        layer_mean_of_activs = [] # Stores average activations per layer\n",
    "        for i in range (layer_output.shape[2]): # loop over each filter (3rd dimention) \n",
    "            filter_mean_activ = layer_output[:,:,i].mean() # Get the mean (A hat, from the paper)\n",
    "            layer_mean_of_activs.append(filter_mean_activ)\n",
    "        layer_max_activ = max(layer_mean_of_activs) # Get max (This is lambda of the layer)\n",
    "        lambdas.append(layer_max_activ)\n",
    "        if(normalize):\n",
    "            layer_mean_of_activs = [x/layer_max_activ for x in layer_mean_of_activs] # Normalize\n",
    "        layers_means.append(layer_mean_of_activs) # This is lambda prime of layer\n",
    "    return layers_means, lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0272631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset should be for one target ethnicity \n",
    "def compute_lambdas(test_samples):\n",
    "    lambdas = []\n",
    "    for i in tqdm(range(len(test_samples))):\n",
    "        image = test_samples[i]\n",
    "        image = np.reshape(image,[1,120,120,3])\n",
    "        all_outputs = activation_model.predict(image)\n",
    "        means,layers_lambdas = compute_avg_activation(all_outputs,True)\n",
    "        lambdas.append(layers_lambdas)\n",
    "    lambdas = np.array(lambdas)\n",
    "    lambdas_max = np.max(lambdas,0) # store the max per layer\n",
    "    lambdas_min = np.min(lambdas,0) # store the min per layer\n",
    "    return lambdas_max,lambdas_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321a959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_max_gray_samples, lambdas_min_gray_samples = compute_lambdas(gray_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a7100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_max_colored_samples, lambdas_min_colored_samples = compute_lambdas(colored_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968c844",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_max_red_samples, lambdas_min_red_samples = compute_lambdas(red_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d93c1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_max_blue_samples, lambdas_min_blue_samples = compute_lambdas(blue_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1952e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambdas_max_green_samples, lambdas_min_green_samples = compute_lambdas(green_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25ce007",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [x for x in range(12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba0263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib.pyplot import figure\n",
    "\n",
    "# figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "# plt.plot(layers, lambdas_max_colored_samples,'orange')\n",
    "# plt.plot(layers, lambdas_max_gray_samples,'black')\n",
    "\n",
    "# plt.plot(layers, lambdas_max_red_samples,'red')\n",
    "# plt.plot(layers, lambdas_max_green_samples,'green')\n",
    "# plt.plot(layers, lambdas_max_blue_samples,'blue')\n",
    "\n",
    "# plt.legend(['Colored',\"Gray\",\"Red\",\"Green\",\"Blue\"])\n",
    "# plt.title('Activations per Category')\n",
    "# plt.xlabel('Layer')\n",
    "# plt.ylabel('Activation Norm')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cb2b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import inf\n",
    "\n",
    "vals_red = lambdas_max_red_samples/lambdas_min_red_samples\n",
    "vals_red[vals_red==inf] = 1.0\n",
    "\n",
    "vals_colored = lambdas_max_colored_samples/lambdas_min_colored_samples\n",
    "vals_colored[vals_colored==inf] = 1.0\n",
    "\n",
    "vals_gray = lambdas_max_gray_samples/lambdas_min_gray_samples\n",
    "vals_gray[vals_gray==inf] = 1.0\n",
    "\n",
    "vals_green = lambdas_max_green_samples/lambdas_min_green_samples\n",
    "vals_green[vals_green==inf] = 1.0\n",
    "\n",
    "vals_blue = lambdas_max_blue_samples/lambdas_min_blue_samples\n",
    "vals_blue[vals_blue==inf] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf892e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "def normal(arr):    \n",
    "    x = arr\n",
    "    norm1 = x / np.linalg.norm(x)\n",
    "    norm2 = normalize(x[:,np.newaxis], axis=0).ravel()\n",
    "    return norm2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.pyplot import figure\n",
    "\n",
    "figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "plt.plot(layers, normal(vals_colored),'orange')\n",
    "# plt.plot(layers, normal(vals_gray),'black')\n",
    "\n",
    "plt.plot(layers, normal(vals_red),'red')\n",
    "plt.plot(layers, normal(vals_green),'green')\n",
    "plt.plot(layers, normal(vals_blue),'blue')\n",
    "\n",
    "plt.legend(['Colored',\"Red\",\"Green\",\"Blue\"])\n",
    "plt.title('Activations per Category')\n",
    "plt.xlabel('Layer')\n",
    "plt.ylabel('Activation Norm')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal(vals_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940b1a08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
